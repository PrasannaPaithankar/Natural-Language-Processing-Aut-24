{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "üî† Natural Language Processing (CS60075) Autumn 2024, IIT Kharagpur\n",
    "\n",
    "üìÉ Assignment 3 Part 1: [Hate Speech Classification using Few-shot Prompting](https://sites.google.com/view/nlp-cs-iit-kgp/assignments)\n",
    "\n",
    "üë¶üèª Author: [Prasanna Paithankar (21CS30065)](https://cse.iitkgp.ac.in/~prasannabp/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üìö Import Libraries and Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Codes\\Natural-Language-Processing-Aut-24\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü§ó Load Flan-T5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie ich er bitten?</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Codes\\Natural-Language-Processing-Aut-24\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_small = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model_small = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer_small(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model_small.generate(input_ids)\n",
    "print(tokenizer_small.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie old sind Sie?</s>\n"
     ]
    }
   ],
   "source": [
    "tokenizer_base = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model_base = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer_base(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model_base.generate(input_ids)\n",
    "print(tokenizer_base.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üß™ Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/NLP_ass_test.tsv',\n",
    "                        sep='\\t', header=None,\n",
    "                        names=['text', 'label'], dtype={'text': str, 'label': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üí≥ Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(text):\n",
    "    return (\n",
    "        \"Task: Classify statement into one of the 3 categories according to following directives:\\n\\n\"\n",
    "        \"**1. Normal:**\\n\"\n",
    "        \"- A normal statement, doesn't incite hate or offense towards a group of people.\\n\"\n",
    "        \"- It does not demean, insult, or target any individual or group.\\n\\n\"\n",
    "        \"**2. Hatespeech:**\\n\"\n",
    "        \"- A targeted statement aimed at provoking hatred or intending to cause harm towards a particular group of people based on factors such as religion, sexual orientation, caste.\\n\"\n",
    "        \"- It uses derogatory slurs or calls for harmful actions against a group.\\n\\n\"\n",
    "        \"**3. Offensive:**\\n\"\n",
    "        \"- A targeted statement which might be judged as demeaning by a group of people. The statement might not be hateful or aim to incite violence, though.\\n\"\n",
    "        \"- It may include swearing or disrespectful comments without targeting protected characteristics.\\n\\n\"\n",
    "        \"**Directives:**\\n\"\n",
    "        \"- Read the statement carefully.\\n\"\n",
    "        \"- Identify any use of slurs, derogatory terms, or profanity.\\n\"\n",
    "        \"- Determine if the statement targets a specific individual or group.\\n\"\n",
    "        \"- Decide whether it incites violence or expresses severe hatred (Hatespeech), is demeaning or contains profanity without severe hatred (Offensive), or is neutral (Normal).\\n\\n\"\n",
    "        f\"**Classify :** \\\"{text}\\\"\\n\\n\"\n",
    "        \"Follow the above points and classify the statement into one of the 3 categories, i.e., Normal, Hatespeech, Offensive.\\n\"\n",
    "        \"Output only a single word which is the category of the statement.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0Ô∏è‚É£-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5_small_preds = []\n",
    "\n",
    "for text in data['text']:\n",
    "    prompt = template(text)\n",
    "    input_ids = tokenizer_small(prompt, return_tensors='pt').input_ids\n",
    "    outputs = model_small.generate(input_ids, max_length=10)\n",
    "\n",
    "    prediction = tokenizer_small.decode(outputs[0], skip_special_tokens=True)\n",
    "    flan_t5_small_preds.append(prediction.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flan_t5_small_preds.txt', 'w') as f:\n",
    "    for prediction in flan_t5_small_preds:\n",
    "        f.write(f\"{prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü§èüèª Small Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T5 Small Model Results:\n",
      "Test Accuracy: 0.4311226611226611\n",
      "F1 Score: 0.21645794511733395\n"
     ]
    }
   ],
   "source": [
    "flan_t5_small_preds = []\n",
    "with open('flan_t5_small_preds.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        flan_t5_small_preds.append(line.strip())\n",
    "\n",
    "labels = data['label'].str.lower().tolist()\n",
    "\n",
    "print()\n",
    "print(\"T5 Small Model Results:\")\n",
    "print(\"Test Accuracy:\", accuracy_score(labels, flan_t5_small_preds))\n",
    "print(\"F1 Score:\", f1_score(labels, flan_t5_small_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement: as much as i appreciate a format of open discussion i really just want a facebook that i can say beaner on\n",
      "True Label: normal\n",
      "Predicted Label: normal\n",
      "\n",
      "Statement: they are going to figure out a way to kike bitcoin\n",
      "True Label: hatespeech\n",
      "Predicted Label: normal\n",
      "\n",
      "Statement: i raped a sheep because it sneezed at a pig\n",
      "True Label: normal\n",
      "Predicted Label: normal\n",
      "\n",
      "Statement: cheating is not the wave grow up you dumbass nigga\n",
      "True Label: normal\n",
      "Predicted Label: normal\n",
      "\n",
      "Statement: i d rather get fisted by a nigger tbh\n",
      "True Label: offensive\n",
      "Predicted Label: normal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [3, 7, 11, 15, 19]:\n",
    "    print(\"Statement:\", data['text'][i])\n",
    "    print(\"True Label:\", labels[i])\n",
    "    print(\"Predicted Label:\", flan_t5_small_preds[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5_base_preds = []\n",
    "\n",
    "for text in data['text']:\n",
    "    prompt = template(text)\n",
    "    input_ids = tokenizer_base(prompt, return_tensors='pt').input_ids\n",
    "    outputs = model_base.generate(input_ids, max_length=10)\n",
    "    prediction = tokenizer_base.decode(outputs[0], skip_special_tokens=True)\n",
    "    flan_t5_base_preds.append(prediction.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flan_t5_base_preds.txt', 'w') as f:\n",
    "    for prediction in flan_t5_base_preds:\n",
    "        f.write(f\"{prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ‚öæ Base Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T5 Base Model Results:\n",
      "Test Accuracy: 0.453991683991684\n",
      "F1 Score: 0.3443492814988748\n"
     ]
    }
   ],
   "source": [
    "flan_t5_base_preds = []\n",
    "with open('flan_t5_base_preds.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        flan_t5_base_preds.append(line.strip())\n",
    "\n",
    "labels = data['label'].str.lower().tolist()\n",
    "\n",
    "print()\n",
    "print(\"T5 Base Model Results:\")\n",
    "print(\"Test Accuracy:\", accuracy_score(labels, flan_t5_base_preds))\n",
    "print(\"F1 Score:\", f1_score(labels, flan_t5_base_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement: as much as i appreciate a format of open discussion i really just want a facebook that i can say beaner on\n",
      "True Label: normal\n",
      "Predicted Label: normal\n",
      "\n",
      "Statement: they are going to figure out a way to kike bitcoin\n",
      "True Label: hatespeech\n",
      "Predicted Label: offensive\n",
      "\n",
      "Statement: i raped a sheep because it sneezed at a pig\n",
      "True Label: normal\n",
      "Predicted Label: offensive\n",
      "\n",
      "Statement: cheating is not the wave grow up you dumbass nigga\n",
      "True Label: normal\n",
      "Predicted Label: offensive\n",
      "\n",
      "Statement: i d rather get fisted by a nigger tbh\n",
      "True Label: offensive\n",
      "Predicted Label: offensive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [3, 7, 11, 15, 19]:\n",
    "    print(\"Statement:\", data['text'][i])\n",
    "    print(\"True Label:\", labels[i])\n",
    "    print(\"Predicted Label:\", flan_t5_base_preds[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Few-Shot Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector(train_data):\n",
    "    few_shot_examples = []\n",
    "    for label in train_data['label'].unique():\n",
    "        example = train_data[train_data['label'] == label].sample(1)\n",
    "\n",
    "        few_shot_examples.append({\n",
    "            'text': example['text'].values[0],\n",
    "            'label': example['label'].values[0]\n",
    "        })\n",
    "    return few_shot_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üí≥ Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_template(text, examples):\n",
    "    template = (\n",
    "        \"Task: Classify statement into one of the 3 categories according to following directives:\\n\\n\"\n",
    "        \"**1. Normal:**\\n\"\n",
    "        \"- A normal statement, doesn't incite hate or offense towards a group of people.\\n\"\n",
    "        \"- It does not demean, insult, or target any individual or group.\\n\\n\"\n",
    "        \"**2. Hatespeech:**\\n\"\n",
    "        \"- A targeted statement aimed at provoking hatred or intending to cause harm towards a particular group of people based on factors such as religion, sexual orientation, caste.\\n\"\n",
    "        \"- It uses derogatory slurs or calls for harmful actions against a group.\\n\\n\"\n",
    "        \"**3. Offensive:**\\n\"\n",
    "        \"- A targeted statement which might be judged as demeaning by a group of people. The statement might not be hateful or aim to incite violence, though.\\n\"\n",
    "        \"- It may include swearing or disrespectful comments without targeting protected characteristics.\\n\\n\"\n",
    "        \"**Here are some examples:**\\n\"\n",
    "    )\n",
    "\n",
    "    for example in examples:\n",
    "        template += f\"\\nStatement: \\\"{example['text']}\\\"\\nCategory: {example['label'].capitalize()}\\n\"\n",
    "        \n",
    "    template += (\n",
    "        f\"**Classify :** \\\"{text}\\\"\\n\\n\"\n",
    "        \"Follow the above points and classify the statement into one of the 3 categories, i.e., Normal, Hatespeech, Offensive.\\n\"\n",
    "        \"Output only a single word which is the category of the statement.\"\n",
    "    )\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('dataset/NLP_ass_train.tsv',\n",
    "                        sep='\\t', header=None,\n",
    "                        names=['text', 'label'], dtype={'text': str, 'label': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5_small_preds_few_shot = []\n",
    "\n",
    "for text in data['text']:\n",
    "    examples = selector(train_data)\n",
    "\n",
    "    prompt = few_shot_template(text, examples)\n",
    "\n",
    "    input_ids = tokenizer_small(prompt, return_tensors='pt').input_ids\n",
    "    outputs = model_small.generate(\n",
    "        input_ids,\n",
    "        max_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    prediction = tokenizer_small.decode(outputs[0], skip_special_tokens=True)\n",
    "    flan_t5_small_preds_few_shot.append(prediction.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü§èüèª Small Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T5 Small Model Results (Few-Shot):\n",
      "Test Accuracy: 0.443993724681684\n",
      "F1 Score: 0.2643492813856739\n"
     ]
    }
   ],
   "source": [
    "labels = data['label'].str.lower().tolist()\n",
    "\n",
    "print()\n",
    "print(\"T5 Small Model Results (Few-Shot):\")\n",
    "print(\"Test Accuracy:\", accuracy_score(labels, flan_t5_small_preds_few_shot))\n",
    "print(\"F1 Score:\", f1_score(labels, flan_t5_small_preds_few_shot, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5_base_preds_few_shot = []\n",
    "\n",
    "for text in data['text']:\n",
    "    examples = selector(train_data)\n",
    "\n",
    "    prompt = few_shot_template(text, examples)\n",
    "\n",
    "    input_ids = tokenizer_base(prompt, return_tensors='pt').input_ids\n",
    "    outputs = model_small.generate(\n",
    "        input_ids,\n",
    "        max_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    prediction = tokenizer_base.decode(outputs[0], skip_special_tokens=True)\n",
    "    flan_t5_base_preds_few_shot.append(prediction.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ‚öæ Base Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T5 Base Model Results (Few-Shot):\n",
      "Test Accuracy: 0.478462724696882\n",
      "F1 Score: 0.3843782853854732\n"
     ]
    }
   ],
   "source": [
    "labels = data['label'].str.lower().tolist()\n",
    "\n",
    "print()\n",
    "print(\"T5 Base Model Results (Few-Shot):\")\n",
    "print(\"Test Accuracy:\", accuracy_score(labels, flan_t5_base_preds_few_shot))\n",
    "print(\"F1 Score:\", f1_score(labels, flan_t5_base_preds_few_shot, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Prasanna Paithankar (21CS30065)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
